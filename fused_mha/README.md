记录一下关于fused multi-head attention的代码阅读。

1. Flash Attention [[codes](https://github.com/HazyResearch/flash-attention)] [[notes](./codes_flash_attention.md)]
