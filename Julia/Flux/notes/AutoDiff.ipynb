{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Mode Auto differentiation in JPL\n",
    "\n",
    "## basics of automatic differentiation\n",
    "\n",
    "1. the input function $f$ itself\n",
    "1. for machine learning optimization, let's focus on a particular case: input functions have _**multidimensional input and return one single output**_:\n",
    "\n",
    "    $$f: \\mathbb{R}^n \\rightarrow \\mathbb{R} $$\n",
    "    *  input function $f$ for AD routines needs to be _**decomposable to a sequence of differentiable ‘elementary’ functions**_\n",
    "    * elementary function means unary/binary operators like $+$,$-$,$*$ and functions from basic calculus, like `sin`, `cos`, `exp`.\n",
    "      \n",
    "    > My question here is,\n",
    "    > * How forward mode differentiation is applied to things beyond unary/binary operators, such as matrix multiplication, tensor computations.\n",
    "    > * Will tensor computations are eventually also decomposed to the elementary functions?\n",
    "    > * If so, will all the optimizations are left to the compiler?\n",
    "    > * Theoretically, is it able to achieve comparable speed as the AD implemented as in TF (it is at a very high-level) if we maximize all the optimizations we can apply.\n",
    "1. differentiation relies on ‘past’ functions’ evaluations\n",
    "\n",
    "## Functions Overloading and Dual Numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
