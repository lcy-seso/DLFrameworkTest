{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Model-Building Basics](https://github.com/FluxML/Flux.jl/blob/master/docs/src/models/basics.md#model-building-basics)\n",
    "\n",
    "## Taking Gradients\n",
    "\n",
    "* ~~_**It seems that Flux mainly use [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) to implement the backward computation of an operator**_.~~\n",
    "  * ~~Will forward AD ad be much slower than the reverse mode AD?~~\n",
    "  \n",
    "  **Flux implements its own reverse-mode AD**.\n",
    "\n",
    "* The [`gradient` function](https://github.com/FluxML/Flux.jl/blob/master/src/tracker/back.jl#L180) takes another Julia function $f$ and a set of arguments, and returns the gradient with respect to each argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 (tracked)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker\n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "df(x) = Tracker.gradient(f, x)[1]\n",
    "d2f(x) = Tracker.gradient(df, x)[1]\n",
    "\n",
    "df(2)\n",
    "d2f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Machine learning models can have hundreds of parameters.\n",
    "* [param](https://github.com/FluxML/Flux.jl/blob/master/src/treelike.jl#L42) tells Flux to treat something as a learnable parameter.\n",
    "* Then we can tell `gradient` to collect the gradients of all of them at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads[W] = 4.0\n",
      "grads[b] = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = param(2)\n",
    "b = param(3)\n",
    "\n",
    "f(x) = W * x + b\n",
    "\n",
    "params = Params([W, b])\n",
    "grads = Tracker.gradient(() -> f(4), params)\n",
    "\n",
    "@show grads[W]\n",
    "@show grads[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tracked things behave like normal numbers or arrays, but keep records of everything you do with them.\n",
    "* `Tracked` allows Flux to calculate gradients of tracked things.\n",
    "* `gradient` takes a zero-argument function\n",
    "  * no arguments are necessary because the Params tell it what to differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear(in, out)\n",
    "  W = param(randn(out, in))\n",
    "  b = param(randn(out))\n",
    "  x -> W * x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the above cell, the return value of the last line: `x -> W * x .+ b` is returned, which is a definition of an anonymous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads[linear1.W] = Flux.Tracker.TrackedReal{Float64}[-5.72233 (tracked) -30.4402 (tracked) -9.67905 (tracked) -8.99108 (tracked) -0.504939 (tracked); 1.4963 (tracked) 7.95964 (tracked) 2.53092 (tracked) 2.35103 (tracked) 0.132034 (tracked); 0.337755 (tracked) 1.7967 (tracked) 0.571297 (tracked) 0.53069 (tracked) 0.0298036 (tracked)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tracked 3Ã—5 Array{Float64,2}:\n",
       " -5.72233   -30.4402   -9.67905   -8.99108  -0.504939 \n",
       "  1.4963      7.95964   2.53092    2.35103   0.132034 \n",
       "  0.337755    1.7967    0.571297   0.53069   0.0298036"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building models by stacking multiple layers.\n",
    "\n",
    "# Define 2 linear layers\n",
    "linear1 = linear(5, 3)\n",
    "linear2 = linear(3, 2)\n",
    "\n",
    "# stack 2 linear layers\n",
    "predict(x) = linear2(linear1(x))\n",
    "\n",
    "# define the loss function\n",
    "x, y = rand(5), rand(2)  # random test data\n",
    "loss(x, y) = sum((predict(x) .- y).^2)\n",
    "\n",
    "# all the learnable parameters in the model\n",
    "params = Params([linear1.W, linear1.b, linear2.W, linear2.b])\n",
    "grads = Tracker.gradient(() -> loss(x, y), params)\n",
    "\n",
    "@show grads[linear1.W]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
